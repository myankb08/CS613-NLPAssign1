{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13618559,"sourceType":"datasetVersion","datasetId":8654739},{"sourceId":13622708,"sourceType":"datasetVersion","datasetId":8657850},{"sourceId":13624990,"sourceType":"datasetVersion","datasetId":8659539}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install essential libraries for deep learning, model loading, and evaluation\n!pip install torch --quiet\n!pip install transformers==4.53.2 --quiet\n!pip install datasets==2.18.0 --quiet\n!pip install evaluate==0.4.1 --quiet\n!pip install sacrebleu==2.4.2 --quiet\n!pip install pandas --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T10:57:25.491899Z","iopub.execute_input":"2025-11-11T10:57:25.492193Z","iopub.status.idle":"2025-11-11T10:59:04.943319Z","shell.execute_reply.started":"2025-11-11T10:57:25.492156Z","shell.execute_reply":"2025-11-11T10:59:04.942454Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m170.9/170.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.2.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Install the custom toolkit required for IndicTrans2 pre-processing and post-processing\n!pip install indictranstoolkit --quiet","metadata":{"execution":{"iopub.status.busy":"2025-11-11T10:59:04.944972Z","iopub.execute_input":"2025-11-11T10:59:04.945279Z","iopub.status.idle":"2025-11-11T10:59:09.082717Z","shell.execute_reply.started":"2025-11-11T10:59:04.945248Z","shell.execute_reply":"2025-11-11T10:59:09.081964Z"},"trusted":true},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m546.1/546.1 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.8/53.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n\n# Access the stored Hugging Face token from Kaggle Secrets\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\n\n# Programmatically log in to Hugging Face\nlogin(token=hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T10:59:09.083896Z","iopub.execute_input":"2025-11-11T10:59:09.084258Z","iopub.status.idle":"2025-11-11T10:59:09.720651Z","shell.execute_reply.started":"2025-11-11T10:59:09.084199Z","shell.execute_reply":"2025-11-11T10:59:09.719906Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nimport pandas as pd\n\n# Define the device for computation (GPU if available, otherwise CPU)\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nLANGS = [\"hin_Deva\", \"tel_Telu\"]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T10:59:09.722393Z","iopub.execute_input":"2025-11-11T10:59:09.722631Z","iopub.status.idle":"2025-11-11T10:59:13.082609Z","shell.execute_reply.started":"2025-11-11T10:59:09.722614Z","shell.execute_reply":"2025-11-11T10:59:13.081834Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\nMODEL_NAME = \"ai4bharat/indictrans2-indic-indic-dist-320M\"\n\n# Load the tokenizer\n# trust_remote_code is required for custom tokenization logic\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n\n# Load the model\n# trust_remote_code is required for custom model architecture\n# torch_dtype=torch.float16 uses half-precision for memory efficiency and speed\nmodel = AutoModelForSeq2SeqLM.from_pretrained(\n    MODEL_NAME,\n    trust_remote_code=True,\n).to(DEVICE)\n\nmodel.eval() # Set the model to evaluation mode","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T10:59:13.083506Z","iopub.execute_input":"2025-11-11T10:59:13.083934Z","iopub.status.idle":"2025-11-11T10:59:41.118000Z","shell.execute_reply.started":"2025-11-11T10:59:13.083910Z","shell.execute_reply":"2025-11-11T10:59:41.117132Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"272106eb4eb1405f90d979f4287bd121"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenization_indictrans.py:   0%|          | 0.00/8.04k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b22c6b9a3324ce39f6c38d58a1bf078"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-indic-indic-dist-320M:\n- tokenization_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"dict.SRC.json:   0%|          | 0.00/3.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02a4019de0324585bc86bad4cba7ff3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"dict.TGT.json:   0%|          | 0.00/3.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6aced90137544cacb8ee36c1b2302f5c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.SRC:   0%|          | 0.00/3.26M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa08802b8457455b9506b43e17d59e8f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/96.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dc44b2075ad421c84a4e29ef1234f97"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"421b8c3467ea4beaaf463a0be2b39b19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"configuration_indictrans.py:   0%|          | 0.00/14.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"763ffe21ec2644d6bacfc51a13d7be93"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-indic-indic-dist-320M:\n- configuration_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modeling_indictrans.py:   0%|          | 0.00/79.8k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3d89a7c38a644aa0a9d79f198da14a57"}},"metadata":{}},{"name":"stderr","text":"A new version of the following files was downloaded from https://huggingface.co/ai4bharat/indictrans2-indic-indic-dist-320M:\n- modeling_indictrans.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n2025-11-11 10:59:24.037655: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762858764.237065      38 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762858764.292709      38 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.28G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea8a2a7429d24bd9a8a68eacc2c01c0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe0e86f5de8f43b8b76d9925b325531b"}},"metadata":{}},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"IndicTransForConditionalGeneration(\n  (model): IndicTransModel(\n    (encoder): IndicTransEncoder(\n      (embed_tokens): Embedding(122706, 512, padding_idx=1)\n      (embed_positions): IndicTransSinusoidalPositionalEmbedding()\n      (layers): ModuleList(\n        (0-17): 18 x IndicTransEncoderLayer(\n          (self_attn): IndicTransAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (activation_fn): GELUActivation()\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n    (decoder): IndicTransDecoder(\n      (embed_tokens): Embedding(122672, 512, padding_idx=1)\n      (embed_positions): IndicTransSinusoidalPositionalEmbedding()\n      (layers): ModuleList(\n        (0-17): 18 x IndicTransDecoderLayer(\n          (self_attn): IndicTransAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (activation_fn): GELUActivation()\n          (self_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (encoder_attn): IndicTransAttention(\n            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n          )\n          (encoder_attn_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n          (fc1): Linear(in_features=512, out_features=2048, bias=True)\n          (fc2): Linear(in_features=2048, out_features=512, bias=True)\n          (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n        )\n      )\n      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n      (layernorm_embedding): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (lm_head): Linear(in_features=512, out_features=122672, bias=False)\n)"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"from IndicTransToolkit.processor import IndicProcessor\n\n# Instantiate the processor for inference tasks\nip = IndicProcessor(inference=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T10:59:41.118945Z","iopub.execute_input":"2025-11-11T10:59:41.119614Z","iopub.status.idle":"2025-11-11T10:59:41.658632Z","shell.execute_reply.started":"2025-11-11T10:59:41.119592Z","shell.execute_reply":"2025-11-11T10:59:41.658072Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def translate_docs(docs, src_lang, tgt_lang):\n    translations = []\n    for idx, doc in enumerate(docs):\n        try:\n            # Use preprocess_batch for batch of one\n            preprocessed = ip.preprocess_batch([doc], src_lang=src_lang, tgt_lang=tgt_lang)\n            if not preprocessed:\n                print(f\"Skipping empty doc at index {idx}\")\n                translations.append(\"\")\n                continue\n        except Exception as e:\n            print(f\"Preprocessing error for index {idx}: {e}\")\n            translations.append(\"\")\n            continue\n        \n        inputs = tokenizer(preprocessed, return_tensors=\"pt\", truncation=True, padding=True).to(DEVICE)\n        forced_bos_token_id = tokenizer.convert_tokens_to_ids(f\"<2{tgt_lang}>\")\n        with torch.no_grad():\n            generated = model.generate(\n                **inputs,\n                num_beams=5,\n                max_length=2048,  # Use higher if doc is large\n                forced_bos_token_id=forced_bos_token_id\n            )\n        decoded = tokenizer.batch_decode(generated, skip_special_tokens=True)\n        try:\n            # Use postprocess_batch for batch of one\n            postprocessed = ip.postprocess_batch(decoded, lang=tgt_lang)\n            translations.append(postprocessed[0] if postprocessed else decoded[0])\n        except Exception as e:\n            print(f\"Postprocessing error for index {idx}: {e}\")\n            translations.append(decoded[0])\n    return translations\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T10:59:41.659299Z","iopub.execute_input":"2025-11-11T10:59:41.659495Z","iopub.status.idle":"2025-11-11T10:59:41.665602Z","shell.execute_reply.started":"2025-11-11T10:59:41.659479Z","shell.execute_reply":"2025-11-11T10:59:41.665075Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# import pandas as pd\n\n# # Load dataset CSV with columns: 'hin_Deva', 'tel_Telu'\n# df = pd.read_csv('/kaggle/input/coi-hin-tel/COI_hin_tel - Sheet1.csv')\n\n# # Translate Telugu to Hindi\n# tel_docs = df['tel_Telu'].tolist()    # Telugu documents for translation\n# translated_hindi = translate_docs(tel_docs, src_lang='tel_Telu', tgt_lang='hin_Deva')\n# df['trans_hindi'] = translated_hindi\n\n# # Translate Hindi to Telugu\n# hin_docs = df['hin_Deva'].tolist()    # Hindi documents for translation\n# translated_telugu = translate_docs(hin_docs, src_lang='hin_Deva', tgt_lang='tel_Telu')\n# df['trans_telugu'] = translated_telugu\n\n\n# # Save results to CSV\n# df.to_csv('/kaggle/working/hindi_telugu_translated_coi.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T10:59:41.666256Z","iopub.execute_input":"2025-11-11T10:59:41.666666Z","iopub.status.idle":"2025-11-11T10:59:41.680552Z","shell.execute_reply.started":"2025-11-11T10:59:41.666644Z","shell.execute_reply":"2025-11-11T10:59:41.679712Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# from sacrebleu.metrics import CHRF\n# import pandas as pd\n\n# def evaluatedirection(srclang, tgtlang, dataset):\n#     \"\"\"\n#     Evaluate translation quality for src to tgt given the dataset.\n#     srclang, tgtlang are column names in the dataset.\n#     dataset is a DataFrame with references for tgtlang.\n#     \"\"\"\n#     # Extract source sentences and target reference sentences\n#     srcsentences = dataset[srclang].tolist()\n#     tgtsentences = dataset[tgtlang].tolist()\n    \n#     # Translate source sentences (using your translation function)\n#     preds = translate_docs(srcsentences, src_lang=srclang, tgt_lang=tgtlang)\n    \n#     # Flatten preds if needed\n#     preds = [p if isinstance(p, str) else p[0] for p in preds]\n#     if len(preds) == 0:\n#         print(f\"No predictions produced for {srclang}-{tgtlang}\")\n#         return None\n    \n#     # Calculate chrF score\n#     chrf = CHRF(word_order=2)\n#     score = chrf.corpus_score(preds, tgtsentences).score\n#     return score\n\n# # Load your dataset CSV with reference column names\n# df = pd.read_csv('/kaggle/input/coi-hin-tel/COI_hin_tel - Sheet1.csv')\n\n# # Evaluate Hindi to Telugu translation quality\n# score_hindi_telugu = evaluatedirection('hin_Deva', 'tel_Telu', df)\n# print(f\"chrF Hindi->Telugu: {score_hindi_telugu:.2f}\")\n\n# # Evaluate Telugu to Hindi translation quality\n# score_telugu_hindi = evaluatedirection('tel_Telu', 'hin_Deva', df)\n# print(f\"chrF Telugu->Hindi: {score_telugu_hindi:.2f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T10:59:41.681307Z","iopub.execute_input":"2025-11-11T10:59:41.681555Z","iopub.status.idle":"2025-11-11T10:59:41.693677Z","shell.execute_reply.started":"2025-11-11T10:59:41.681539Z","shell.execute_reply":"2025-11-11T10:59:41.693087Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nimport random\nfrom datasets import load_dataset\nfrom sacrebleu.metrics import CHRF\n\n# --- 1. Load LTRC Dataset ---\n# We load 'train' and take a slice. \n# We'll fetch 3000 sentences to make ~200-250 paragraphs.\n# Running on the full 700k+ sentences will be too slow for a notebook.\nNUM_SENTENCES_TO_FETCH = 3000\nK_MIN, K_MAX = 8, 14  # Your range of 8-14 sentences per paragraph\n\nprint(\"Loading LTRC dataset from Hugging Face...\")\ntry:\n    ds = load_dataset(\"ltrciiith/The-LTRC-Hindi-Telugu-Parallel-Corpus\", split=f'train[:{NUM_SENTENCES_TO_FETCH}]')\n    print(f\"Loaded {len(ds)} sentences.\")\nexcept Exception as e:\n    print(f\"Failed to load dataset: {e}\")\n    print(\"Please ensure you have internet access enabled in your Kaggle notebook.\")\n    # Stop execution if dataset fails\n    raise e\n\n# --- 2. Function to Create Paragraphs ---\ndef create_paragraph_dataset(dataset, k_min, k_max):\n    paragraphs = []\n    # Get the raw sentences from the Hugging Face dataset\n    all_hin_sents = dataset['source_text']\n    all_tel_sents = dataset['target_text']\n    \n    total_sents = len(all_hin_sents)\n    current_index = 0\n    \n    print(f\"Creating parallel paragraphs from {total_sents} sentences...\")\n    \n    while current_index < total_sents:\n        # Get a random paragraph size\n        k = random.randint(k_min, k_max)\n        \n        # Ensure we don't go out of bounds\n        if current_index + k > total_sents:\n            k = total_sents - current_index\n            if k == 0:\n                break\n        \n        # Get the slices of sentences for this paragraph\n        hin_sents_slice = all_hin_sents[current_index : current_index + k]\n        tel_sents_slice = all_tel_sents[current_index : current_index + k]\n        \n        # Join with a space to form a single paragraph string\n        hin_para = \" \".join(hin_sents_slice)\n        tel_para = \" \".join(tel_sents_slice)\n        \n        # Add to our list with the column names your functions expect\n        paragraphs.append({\n            \"hin_Deva\": hin_para,\n            \"tel_Telu\": tel_para\n        })\n        \n        # Move the index to the start of the next paragraph\n        current_index += k\n    \n    print(f\"Successfully created {len(paragraphs)} paragraphs.\")\n    # Convert the list of dicts into the DataFrame your code expects\n    return pd.DataFrame(paragraphs)\n\n# --- 3. Create the Paragraph DataFrame ---\n# This df will have two columns: 'hin_Deva' and 'tel_Telu'\n# Each row contains one full paragraph (of 8-14 sentences).\ndf = create_paragraph_dataset(ds, k_min=K_MIN, k_max=K_MAX)\n\n# --- 4. Your Evaluation Function (Unchanged) ---\n# This is your exact function from cell 10. It will now get\n# the paragraph-based DataFrame 'df' passed into it.\ndef evaluatedirection(srclang, tgtlang, dataset):\n    \"\"\"\n    Evaluate translation quality for src to tgt given the dataset.\n    srclang, tgtlang are column names in the dataset.\n    dataset is a DataFrame with references for tgtlang.\n    \"\"\"\n    print(f\"\\n--- Starting Evaluation: {srclang} -> {tgtlang} ---\")\n    \n    # Extract source sentences (paragraphs in this case)\n    srcsentences = dataset[srclang].tolist()\n    # Extract target reference sentences (paragraphs)\n    tgtsentences = dataset[tgtlang].tolist()\n    \n    if not srcsentences or not tgtsentences:\n        print(\"Error: Empty source or target sentences. Check your DataFrame.\")\n        return None\n        \n    print(f\"Translating {len(srcsentences)} paragraphs...\")\n    \n    # Translate source paragraphs using your 'translate_docs' function from cell 8\n    # This function is already set up to handle long inputs (max_length=2048)\n    preds = translate_docs(srcsentences, src_lang=srclang, tgt_lang=tgtlang)\n    \n    # Flatten preds if needed\n    preds = [p if isinstance(p, str) else p[0] for p in preds]\n    if len(preds) == 0:\n        print(f\"No predictions produced for {srclang}-{tgtlang}\")\n        return None\n    \n    print(\"Translation complete. Calculating chrF++ score...\")\n    \n    # Calculate chrF score\n    chrf = CHRF(word_order=2)\n    # Note: sacrebleu's corpus_score expects references in a list of lists\n    # So we wrap tgtsentences: [tgtsentences]\n    score = chrf.corpus_score(preds, [tgtsentences]).score\n    return score\n\n# --- 5. Run the Final Evaluation ---\n# This now runs the evaluation on your new paragraph-level dataset\n\n# Evaluate Hindi to Telugu translation quality\nscore_hindi_telugu = evaluatedirection('hin_Deva', 'tel_Telu', df)\nprint(f\"----------------------------------------------------------\")\nprint(f\"FINAL chrF Hindi->Telugu (Paragraphs): {score_hindi_telugu:.2f}\")\nprint(f\"----------------------------------------------------------\")\n\n\n# Evaluate Telugu to Hindi translation quality\nscore_telugu_hindi = evaluatedirection('tel_Telu', 'hin_Deva', df)\nprint(f\"----------------------------------------------------------\")\nprint(f\"FINAL chrF Telugu->Hindi (Paragraphs): {score_telugu_hindi:.2f}\")\nprint(f\"----------------------------------------------------------\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-11T11:02:37.335834Z","iopub.execute_input":"2025-11-11T11:02:37.336538Z","iopub.status.idle":"2025-11-11T11:37:11.063460Z","shell.execute_reply.started":"2025-11-11T11:02:37.336512Z","shell.execute_reply":"2025-11-11T11:37:11.062631Z"}},"outputs":[{"name":"stdout","text":"Loading LTRC dataset from Hugging Face...\nLoaded 3000 sentences.\nCreating parallel paragraphs from 3000 sentences...\nSuccessfully created 270 paragraphs.\n\n--- Starting Evaluation: hin_Deva -> tel_Telu ---\nTranslating 270 paragraphs...\nTranslation complete. Calculating chrF++ score...\n----------------------------------------------------------\nFINAL chrF Hindi->Telugu (Paragraphs): 31.42\n----------------------------------------------------------\n\n--- Starting Evaluation: tel_Telu -> hin_Deva ---\nTranslating 270 paragraphs...\nTranslation complete. Calculating chrF++ score...\n----------------------------------------------------------\nFINAL chrF Telugu->Hindi (Paragraphs): 34.26\n----------------------------------------------------------\n","output_type":"stream"}],"execution_count":11}]}